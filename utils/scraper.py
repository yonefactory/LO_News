import json
import os
import requests
from bs4 import BeautifulSoup
from utils.summarizer import translate_title, summarize_article

SENT_ARTICLES_FILE = "sent_articles.json"

def load_sent_articles():
    """ì „ì†¡ëœ ê¸°ì‚¬ ëª©ë¡ì„ JSON íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°"""
    if os.path.exists(SENT_ARTICLES_FILE):
        with open(SENT_ARTICLES_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_sent_articles(sent_articles):
    """ì „ì†¡ëœ ê¸°ì‚¬ ëª©ë¡ì„ JSON íŒŒì¼ì— ì €ì¥"""
    with open(SENT_ARTICLES_FILE, "w", encoding="utf-8") as f:
        json.dump(sent_articles, f, ensure_ascii=False, indent=4)

def fetch_news_from_site(url, title_selector, link_selector, base_url="", limit=1):
    """íŠ¹ì • ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ë‰´ìŠ¤ í¬ë¡¤ë§ (í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” limit=1 ì ìš©)"""
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    articles = []

    for link_tag in soup.select(link_selector):
        title = link_tag.text.strip()
        link = link_tag.get("href", "")
        if not link.startswith("http"):
            link = base_url + link
        translated_title = translate_title(title)
        summary = summarize_article(link)
        articles.append({"title": translated_title, "link": link, "summary": summary})

        if len(articles) >= limit:  # âœ… í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” ìµœê·¼ 1ê°œë§Œ ê°€ì ¸ì˜¤ê¸°
            break

    return articles

def get_latest_news(test_mode=False):
    """ìµœì‹  ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ë°©ì§€)"""
    news_sources = [
        {"url": "https://9to5mac.com/", "title_selector": "a.title", "link_selector": "a.title", "base_url": ""},
        {"url": "https://www.macrumors.com/", "title_selector": "a.title", "link_selector": "a.title", "base_url": "https://www.macrumors.com"},
        {"url": "https://www.apple.com/kr/newsroom/", "title_selector": "a.headline", "link_selector": "a.headline", "base_url": "https://www.apple.com"},
        {"url": "https://kr.investing.com/equities/apple-computer-inc-news", "title_selector": "a.title", "link_selector": "a.title", "base_url": "https://kr.investing.com"}
    ]

    all_articles = []
    sent_articles = load_sent_articles()  # âœ… ì´ë¯¸ ë³´ë‚¸ ê¸°ì‚¬ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°

    for source in news_sources:
        print(f"ğŸ” [DEBUG] '{source['url']}' ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ ì¶”ì¶œ ì‹œì‘...")
        articles = fetch_news_from_site(
            source["url"],
            source["title_selector"],
            source["link_selector"],
            source["base_url"],
            limit=1 if test_mode else 5  # âœ… í…ŒìŠ¤íŠ¸ ëª¨ë“œì¼ ë•ŒëŠ” 1ê°œë§Œ ê°€ì ¸ì˜¤ê¸°
        )
        
        print(f"ğŸ“° [DEBUG] '{source['url']}' ì‚¬ì´íŠ¸ì—ì„œ {len(articles)}ê°œì˜ ê¸°ì‚¬ ì¶”ì¶œ ì™„ë£Œ:")
        for article in articles:
            print(f"    - ì œëª©: {article['title']}")
            print(f"      ë§í¬: {article['link']}")
            if article["link"] not in sent_articles:  # âœ… ì´ë¯¸ ë³´ë‚¸ ê¸°ì‚¬ì™€ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ í•„í„°ë§
                sent_articles.append(article["link"])
                all_articles.append(article)

    save_sent_articles(sent_articles)  # âœ… ìƒˆë¡œìš´ ê¸°ì‚¬ ì €ì¥
    return all_articles
