import json
import os
import requests
import feedparser  # RSS í”¼ë“œ íŒŒì‹±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from bs4 import BeautifulSoup
from utils.summarizer import translate_title, summarize_article

SENT_ARTICLES_FILE = "sent_articles.json"

def load_sent_articles():
    """ì „ì†¡ëœ ê¸°ì‚¬ ëª©ë¡ì„ JSON íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°"""
    if os.path.exists(SENT_ARTICLES_FILE):
        with open(SENT_ARTICLES_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_sent_articles(sent_articles):
    """ì „ì†¡ëœ ê¸°ì‚¬ ëª©ë¡ì„ JSON íŒŒì¼ì— ì €ì¥"""
    with open(SENT_ARTICLES_FILE, "w", encoding="utf-8") as f:
        json.dump(sent_articles, f, ensure_ascii=False, indent=4)

def fetch_news_from_site(url, title_selector, link_selector, base_url="", limit=1):
    """ì¼ë°˜ì ì¸ ì›¹ í¬ë¡¤ë§ ë°©ì‹ìœ¼ë¡œ ìµœì‹  ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    articles = []

    for link_tag in soup.select(link_selector):
        title = link_tag.text.strip()
        link = link_tag.get("href", "")
        if not link.startswith("http"):
            link = base_url + link
        translated_title = translate_title(title)
        summary = summarize_article(link)
        articles.append({"title": translated_title, "link": link, "summary": summary})

        if len(articles) >= limit:
            break

    return articles

def fetch_news_from_rss(rss_url, limit=1):
    """RSS í”¼ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    feed = feedparser.parse(rss_url)
    articles = []

    for entry in feed.entries[:limit]:  # limit ê°œìˆ˜ë§Œí¼ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
        title = entry.title
        link = entry.link
        translated_title = translate_title(title)
        summary = summarize_article(link)

        articles.append({"title": translated_title, "link": link, "summary": summary})

    return articles

def get_latest_news(test_mode=False):
    """ìµœì‹  ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì¤‘ë³µ ë°©ì§€)"""
    news_sources = [
        {"url": "https://9to5mac.com/", "title_selector": "h2 a", "link_selector": "h2 a", "base_url": "", "use_rss": False},
        {"url": "https://www.macrumors.com/", "rss_url": "https://www.macrumors.com/macrumors.xml", "use_rss": True},
    ]

    all_articles = []
    sent_articles = load_sent_articles()  # âœ… ì´ë¯¸ ë³´ë‚¸ ê¸°ì‚¬ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°

    for source in news_sources:
        print(f"ğŸ” [DEBUG] '{source['url']}' ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ ì¶”ì¶œ ì‹œì‘...")

        if source.get("use_rss"):
            articles = fetch_news_from_rss(source["rss_url"], limit=1 if test_mode else 5)
        else:
            articles = fetch_news_from_site(
                source["url"],
                source["title_selector"],
                source["link_selector"],
                source["base_url"],
                limit=1 if test_mode else 5
            )
        
        print(f"ğŸ“° [DEBUG] '{source['url']}' ì‚¬ì´íŠ¸ì—ì„œ {len(articles)}ê°œì˜ ê¸°ì‚¬ ì¶”ì¶œ ì™„ë£Œ:")
        for article in articles:
            print(f"    - ì œëª©: {article['title']}")
            print(f"      ë§í¬: {article['link']}")
            if article["link"] not in sent_articles:  # âœ… ì´ë¯¸ ë³´ë‚¸ ê¸°ì‚¬ì™€ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ í•„í„°ë§
                sent_articles.append(article["link"])
                all_articles.append(article)

    save_sent_articles(sent_articles)  # âœ… ìƒˆë¡œìš´ ê¸°ì‚¬ ì €ì¥
    return all_articles
